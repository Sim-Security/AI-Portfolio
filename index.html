<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Adam Simonar | AI Engineer</title>
    <meta name="description" content="AI Engineer specializing in multi-agent systems, RAG architectures, and LLM applications. Building production-ready AI agents.">
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
</head>
<body>
    <nav class="nav">
        <div class="nav-content">
            <a href="#" class="nav-logo">AS</a>
            <div class="nav-links">
                <a href="#projects">Projects</a>
                <a href="#blog">Blog</a>
                <a href="#about">About</a>
                <a href="#contact">Contact</a>
            </div>
        </div>
    </nav>

    <header class="hero">
        <div class="hero-content">
            <div class="hero-badge">Available for AI/ML Engineering Roles</div>
            <h1>Adam Simonar</h1>
            <p class="hero-subtitle">AI Engineer</p>
            <p class="hero-description">
                Specializing in <span class="highlight">multi-agent systems</span>,
                <span class="highlight">RAG architectures</span>, and
                <span class="highlight">LLM applications</span>.
                Building production-ready AI agents with LangChain, LangGraph, and modern AI frameworks.
            </p>
            <div class="hero-links">
                <a href="https://github.com/Sim-Security" target="_blank" class="btn btn-primary">
                    <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
                    GitHub
                </a>
                <a href="https://ca.linkedin.com/in/adam-simonar-4b266417b" target="_blank" class="btn btn-secondary">
                    <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg>
                    LinkedIn
                </a>
            </div>
        </div>
    </header>

    <section id="projects" class="projects">
        <div class="section-header">
            <h2>Featured Projects</h2>
            <p>Production-ready AI systems demonstrating multi-agent architectures, RAG patterns, and LLM applications</p>
        </div>

        <div class="project-grid">
            <article class="project-card featured">
                <div class="project-header">
                    <span class="project-tag">Robotics</span>
                    <span class="project-tag">LLM Code Gen</span>
                </div>
                <h3>LLM Robot Arm Simulation</h3>
                <p>Natural language controlled robot arm using the Code-as-Policies pattern. Type commands like "pick up the red cube" or "stack red on blue" - an LLM generates executable Python code, a 4-layer safety system validates it, and PyBullet executes the motion. Features response caching with 4.7M x speedup.</p>
                <div class="project-tech">
                    <span>PyBullet</span>
                    <span>OpenAI</span>
                    <span>Code-as-Policies</span>
                    <span>AST Validation</span>
                    <span>Python</span>
                </div>
                <div class="project-links">
                    <a href="https://github.com/Sim-Security/voice-robot-arm-sim" target="_blank">GitHub</a>
                    <a href="https://github.com/Sim-Security/voice-robot-arm-sim#demo" target="_blank">Watch Demo</a>
                </div>
            </article>

            <article class="project-card featured">
                <div class="project-header">
                    <span class="project-tag">Voice AI</span>
                    <span class="project-tag">Production CI/CD</span>
                </div>
                <h3>AI Lead Qualifier</h3>
                <p>AI voice agent that calls leads within seconds of form submission. Conducts BANT qualification interviews, analyzes transcripts with Claude, and scores leads as hot/warm/cold automatically. Production-ready with GitHub Container Registry auto-deployment.</p>
                <div class="project-tech">
                    <span>Vapi.ai</span>
                    <span>Claude</span>
                    <span>Hono</span>
                    <span>Bun</span>
                    <span>React</span>
                    <span>PostgreSQL</span>
                    <span>GHCR</span>
                    <span>Docker</span>
                </div>
                <div class="project-links">
                    <a href="https://github.com/Sim-Security/AI-Lead-Qualifier" target="_blank">GitHub</a>
                </div>
            </article>

            <article class="project-card featured">
                <div class="project-header">
                    <span class="project-tag">Agentic AI</span>
                    <span class="project-tag">Live Demo</span>
                </div>
                <h3>Codebase Onboarding Agent</h3>
                <p>AI agent that helps developers understand unfamiliar codebases in minutes. 8 deterministic tools for code exploration, 96.7% accuracy across 10 repos in 5 languages, 0% hallucination rate. Deployed on Hugging Face Spaces.</p>
                <div class="project-tech">
                    <span>LangGraph</span>
                    <span>OpenRouter</span>
                    <span>Gradio</span>
                    <span>HuggingFace</span>
                </div>
                <div class="project-links">
                    <a href="https://huggingface.co/spaces/Sim-Security/codebase-onboarding-agent" target="_blank">Live Demo</a>
                    <a href="https://github.com/Sim-Security/codebase-onboarding-agent" target="_blank">GitHub</a>
                </div>
            </article>

            <article class="project-card featured">
                <div class="project-header">
                    <span class="project-tag">Multimodal RAG</span>
                    <span class="project-tag">Human-in-the-Loop</span>
                </div>
                <h3>Vision RAG Agent</h3>
                <p>Multimodal RAG system for understanding and querying images with self-correction and human-in-the-loop patterns. Multi-image comparison with LLM-classified queries and LangGraph interrupts for human approval.</p>
                <div class="project-tech">
                    <span>LangGraph</span>
                    <span>Cohere Embed-4</span>
                    <span>ChromaDB</span>
                    <span>OpenRouter</span>
                    <span>Streamlit</span>
                </div>
                <div class="project-links">
                    <a href="https://github.com/Sim-Security/vision-rag" target="_blank">View on GitHub</a>
                </div>
            </article>

            <article class="project-card featured">
                <div class="project-header">
                    <span class="project-tag">Voice AI</span>
                    <span class="project-tag">100% Local</span>
                </div>
                <h3>Benedict - Local Voice Dictation</h3>
                <p>100% local voice dictation system with real-time transcription and LLM-powered text cleaning. Removes fillers, fixes grammar, with multiple output modes (clean, rewrite, bullets, email).</p>
                <div class="project-tech">
                    <span>RealtimeSTT</span>
                    <span>Whisper</span>
                    <span>Ollama</span>
                    <span>LangChain</span>
                    <span>Streamlit</span>
                </div>
                <div class="project-links">
                    <a href="https://github.com/Sim-Security/Benedict-Local-Voice-Dictation" target="_blank">View on GitHub</a>
                </div>
            </article>

            <article class="project-card">
                <div class="project-header">
                    <span class="project-tag">RAG</span>
                    <span class="project-tag">Self-Correction</span>
                </div>
                <h3>Self-Correcting RAG Agent</h3>
                <p>Advanced retrieval-augmented generation with hallucination detection and self-correction. Automatic verification against source documents with web search fallback.</p>
                <div class="project-tech">
                    <span>LangGraph</span>
                    <span>Claude 3.5</span>
                    <span>Qdrant</span>
                    <span>Streamlit</span>
                </div>
                <div class="project-links">
                    <a href="https://github.com/Sim-Security/Self-Rag-Learning" target="_blank">View on GitHub</a>
                </div>
            </article>

            <article class="project-card">
                <div class="project-header">
                    <span class="project-tag">Multi-Agent</span>
                    <span class="project-tag">Research</span>
                </div>
                <h3>Deep Research Agent</h3>
                <p>Parallel research execution with supervisor-researcher architecture. Multi-model support via OpenRouter with automatic citation tracking and full LangSmith observability.</p>
                <div class="project-tech">
                    <span>LangGraph</span>
                    <span>OpenRouter</span>
                    <span>Tavily</span>
                    <span>LangSmith</span>
                </div>
                <div class="project-links">
                    <a href="https://github.com/Sim-Security/Deep-Research-Agent" target="_blank">View on GitHub</a>
                </div>
            </article>

            <article class="project-card">
                <div class="project-header">
                    <span class="project-tag">Multi-Agent</span>
                    <span class="project-tag">MCP</span>
                </div>
                <h3>AI Teaching Agent Team</h3>
                <p>Multi-agent education system with Professor, Academic Advisor, Research Librarian, and Teaching Assistant agents. Creates comprehensive learning packages with Google Docs integration.</p>
                <div class="project-tech">
                    <span>LangGraph</span>
                    <span>LangChain</span>
                    <span>MCP/Composio</span>
                    <span>Google Docs API</span>
                </div>
                <div class="project-links">
                    <a href="https://github.com/Sim-Security/ai-teaching-agent-team" target="_blank">View on GitHub</a>
                </div>
            </article>

            <article class="project-card">
                <div class="project-header">
                    <span class="project-tag">Fine-Tuning</span>
                    <span class="project-tag">QLoRA</span>
                </div>
                <h3>RegexGPT</h3>
                <p>Fine-tuned LLM converting natural language to regex patterns. QLoRA fine-tuning of Mistral 7B on ~7,000 NL-to-regex pairs with functional evaluation and live testing.</p>
                <div class="project-tech">
                    <span>Transformers</span>
                    <span>PEFT</span>
                    <span>TRL</span>
                    <span>QLoRA</span>
                    <span>Gradio</span>
                </div>
                <div class="project-links">
                    <a href="https://github.com/Sim-Security/Regex-GPT" target="_blank">View on GitHub</a>
                </div>
            </article>

        </div>
    </section>

    <section id="skills" class="skills">
        <div class="section-header">
            <h2>Technical Skills</h2>
        </div>
        <div class="skills-grid">
            <div class="skill-category">
                <h3>AI/ML Engineering</h3>
                <ul>
                    <li>LangChain, LangGraph, LangSmith</li>
                    <li>RAG Systems (Self-RAG, Corrective RAG)</li>
                    <li>Multi-Agent Architectures</li>
                    <li>LLM Fine-tuning (QLoRA, PEFT)</li>
                    <li>Vector Databases (Qdrant, ChromaDB)</li>
                    <li>Robotics Simulation (PyBullet)</li>
                </ul>
            </div>
            <div class="skill-category">
                <h3>LLM Providers</h3>
                <ul>
                    <li>Anthropic Claude</li>
                    <li>OpenAI GPT-4</li>
                    <li>Google Gemini</li>
                    <li>xAI Grok</li>
                    <li>OpenRouter, Ollama</li>
                </ul>
            </div>
            <div class="skill-category">
                <h3>Development</h3>
                <ul>
                    <li>Python, TypeScript</li>
                    <li>React, Next.js</li>
                    <li>Streamlit, Gradio</li>
                    <li>FastAPI, Flask</li>
                    <li>Git/GitHub</li>
                </ul>
            </div>
            <div class="skill-category">
                <h3>Security</h3>
                <ul>
                    <li>OSCP Certified</li>
                    <li>CompTIA Security+</li>
                    <li>CompTIA Network+</li>
                    <li>Blockchain Security</li>
                </ul>
            </div>
        </div>
    </section>

    <section id="blog" class="blog">
        <div class="section-header">
            <h2>Blog</h2>
            <p>Thoughts on AI engineering, agent architectures, and lessons learned building production systems</p>
        </div>
        <div class="blog-grid">
            <article class="blog-card">
                <div class="blog-meta">
                    <span class="blog-date">January 20, 2026</span>
                    <span class="blog-tag">Voice AI</span>
                </div>
                <h3>Building an AI Voice Agent That Calls Leads in 20 Seconds</h3>
                <img src="lead-response-time-infographic.jpg" alt="Lead response time impact on conversion rates" class="blog-image">
                <p class="blog-excerpt">There's a brutal reality in sales: wait 5 minutes to call a lead and your odds of qualifying them drop by 80%. I built an AI voice agent that makes the first call within seconds of form submission, conducts a full qualification interview, and scores the lead automatically.</p>
                <div class="blog-content">
                    <h4>The Speed-to-Lead Problem</h4>
                    <p>I've watched this happen too many times. Someone fills out a contact form, genuinely interested. An hour later, they've moved on. A day later, they can't remember why they reached out. The data backs this up:</p>
                    <ul>
                        <li><strong>Within 5 minutes:</strong> 100x more likely to qualify the lead</li>
                        <li><strong>After 5 minutes:</strong> 80% drop in qualification odds</li>
                        <li><strong>Within 20 seconds:</strong> 220% increase in conversions</li>
                    </ul>
                    <p>Humans can't respond in 20 seconds. Not consistently. Not at scale. So I built something that can.</p>

                    <h4>What AI Lead Qualifier Does</h4>
                    <p>The system is straightforward: when someone submits a lead form, an AI voice agent calls them immediately. Not a robocall script—an actual conversational AI that conducts a BANT qualification interview.</p>
                    <p>BANT stands for Budget, Authority, Need, and Timeline. It's the standard framework for determining if a lead is worth pursuing:</p>
                    <ul>
                        <li><strong>Budget:</strong> "What budget range have you allocated for this?"</li>
                        <li><strong>Authority:</strong> "Who else would be involved in evaluating solutions?"</li>
                        <li><strong>Need:</strong> "What's driving your interest right now?"</li>
                        <li><strong>Timeline:</strong> "When are you looking to have something in place?"</li>
                    </ul>
                    <p>The AI asks these questions naturally, responds to the lead's answers, and extracts the qualification signals. After the call ends, it analyzes the transcript and scores the lead as hot, warm, or cold.</p>
                    <img src="ai-lead-qualifier-architecture.jpg" alt="AI Lead Qualifier system architecture" class="blog-image" style="margin: 1.5rem 0;">

                    <h4>The Architecture</h4>
                    <p>I built this as a TypeScript monorepo with three main components:</p>
                    <ul>
                        <li><strong>Backend (Hono):</strong> Handles form submissions, Vapi webhooks, and transcript analysis. Runs on Bun for speed.</li>
                        <li><strong>Frontend (React + Vite):</strong> Lead form with country code selector, real-time dashboard showing hot/warm/cold distribution, detailed lead view with full transcripts.</li>
                        <li><strong>Database (PostgreSQL + Drizzle):</strong> Stores leads, call metadata, transcripts, and qualification scores with proper indexing.</li>
                    </ul>
                    <p>The voice AI runs on Vapi.ai—they handle the telephony, speech-to-text, and text-to-speech. I send them a transient assistant configuration with the BANT system prompt, and they handle the call. When it ends, they hit my webhook with the full transcript.</p>

                    <h4>The Intelligence Layer</h4>
                    <p>Here's where it gets interesting. A transcript is just text. The intelligence comes from Claude (Anthropic's API) analyzing that transcript to extract structured qualification data.</p>
                    <p>The AI doesn't just look for keywords. It understands context:</p>
                    <ul>
                        <li>A call that ends in 30 seconds? Probably a hangup. Score: cold.</li>
                        <li>Silence timeout? They stopped engaging. Score: cold.</li>
                        <li>Voicemail detected? No conversation happened. Score: cold.</li>
                        <li>Full conversation with timeline urgency? Score: hot.</li>
                    </ul>
                    <p>This call-context awareness was critical. Without it, a polite voicemail greeting would score the same as a genuine interest conversation.</p>

                    <h4>Key Technical Decisions</h4>
                    <p><strong>Transient Assistants:</strong> Instead of pre-creating Vapi assistants, I send the full configuration (system prompt, voice settings, model) with each call. This means the prompt can be customized per-lead or per-campaign without managing assistant state.</p>
                    <p><strong>BYOK Architecture:</strong> The system supports Bring Your Own Keys. API keys can come from environment variables (production) or a Settings UI (development). No secrets in code, no vendor lock-in.</p>
                    <p><strong>Auto-Migration:</strong> Database migrations run automatically on backend startup. No manual setup steps, no "forgot to run migrations" bugs.</p>
                    <p><strong>E.164 Phone Format:</strong> International phone numbers need proper formatting. The frontend includes a country code selector that ensures numbers are always in E.164 format (+1XXXXXXXXXX) before hitting the API.</p>

                    <h4>Cost Per Call</h4>
                    <p>The economics work out to roughly <strong>$0.35 per call</strong>:</p>
                    <ul>
                        <li>Vapi platform: ~$0.05</li>
                        <li>Deepgram (speech-to-text): ~$0.01</li>
                        <li>GPT-4 (conversation): ~$0.02-0.20 depending on length</li>
                        <li>ElevenLabs (voice): ~$0.036</li>
                    </ul>
                    <p>Compare that to a human SDR's time. Even at minimum wage, a 5-minute call costs more—and you can't make 100 of them simultaneously.</p>

                    <h4>What I'd Do Differently</h4>
                    <p>A few things I'd reconsider in v2:</p>
                    <ul>
                        <li><strong>Claude for the voice agent itself:</strong> Currently using GPT-4 via Vapi for the actual conversation, Claude for transcript analysis. Vapi now supports Claude—worth testing for consistency.</li>
                        <li><strong>Voicemail detection + callback scheduling:</strong> Right now voicemails just score as cold. A smarter system would detect voicemail, schedule a retry, and track attempts.</li>
                        <li><strong>CRM integration:</strong> Leads live in PostgreSQL. In production, you'd want these flowing into Salesforce, HubSpot, or Pipedrive with automatic field mapping.</li>
                    </ul>

                    <h4>Try It Yourself</h4>
                    <p>The project is open source. You'll need accounts with Vapi.ai and Anthropic, but beyond that it's plug and play:</p>
                    <pre><code>git clone https://github.com/Sim-Security/AI-Lead-Qualifier
cd ai-lead-qualifier
cp .env.example .env
# Add your API keys
docker compose up -d</code></pre>
                    <p>Dashboard runs on localhost. Submit a lead with your phone number, and you'll get a call within seconds.</p>
                    <p><a href="https://github.com/Sim-Security/AI-Lead-Qualifier" target="_blank">View on GitHub →</a></p>

                    <h4>The Takeaway</h4>
                    <p>Speed matters in sales. The data is clear: respond in seconds, not minutes. AI voice agents make that possible at scale, with consistent qualification frameworks and automatic scoring.</p>
                    <p>This isn't about replacing salespeople. It's about making sure qualified leads actually reach them—warm, scored, and ready to close.</p>
                </div>
                <a href="#" class="blog-read-more" onclick="toggleBlogPost(event)">Read Full Post</a>
            </article>

            <article class="blog-card">
                <div class="blog-meta">
                    <span class="blog-date">January 14, 2026</span>
                    <span class="blog-tag">AI Protocols</span>
                </div>
                <h3>MCP vs A2A: The Difference Between Having Tools and Hiring Specialists</h3>
                <img src="mcp-vs-a2a-infographic.jpg" alt="MCP vs A2A protocol comparison infographic" class="blog-image">
                <p class="blog-excerpt">Two protocols are shaping how AI agents interact with the world: MCP gives agents tools to do work themselves, while A2A lets agents delegate to other agents entirely. Understanding the difference clicked for me when I thought about it like visiting a hardware store.</p>
                <div class="blog-content">
                    <h4>The Hardware Store Analogy</h4>
                    <p>I've been building with both MCP (Model Context Protocol) and A2A (Agent2Agent Protocol), and for a while I couldn't articulate why they felt so different. Then I thought about hardware stores.</p>
                    <p><strong>MCP is like walking into Home Depot and finding the nail gun yourself.</strong> You have skills: you can walk, read aisle signs, ask the kiosk, and pay at checkout. These are your "tools." You navigate the store, locate what you need, and handle the transaction. You need to understand the store's layout, and if you want something from Lowe's next, you learn a whole new layout.</p>
                    <p><strong>A2A is like asking an employee to get you a nail gun.</strong> You describe what you need ("I'm doing framing work, need something reliable"). The employee knows the inventory, the layout, maybe even that one model has recalls. They handle everything internally - you just get the result. And here's the key: you can ask any store's employee the same way. The protocol for "asking a human for help" is universal.</p>
                    <img src="hardware-store-analogy.jpg" alt="Hardware store analogy for MCP vs A2A" class="blog-image" style="margin: 1.5rem 0;">
                    <p>This isn't just a cute analogy. It maps directly to how these protocols work.</p>

                    <h4>What MCP Actually Does</h4>
                    <p>First, let's be clear: "giving an agent tools" is just basic agentic behavior. Any framework can let an agent call functions. MCP is something more specific.</p>
                    <p>MCP (Model Context Protocol), developed by Anthropic, is a <strong>standardized protocol</strong> for connecting agents to external services. Think of it like USB for AI - a universal plug that works with any compatible service.</p>
                    <p>Without MCP, connecting an agent to Postgres requires custom code. Connecting to Slack requires different custom code. Every integration is bespoke. With MCP:</p>
                    <ul>
                        <li><strong>Discovery</strong> - Agent asks "what can you do?" and gets a capability list</li>
                        <li><strong>Schema</strong> - Tools have typed inputs/outputs, automatically understood</li>
                        <li><strong>Resources</strong> - Access to data, not just functions</li>
                        <li><strong>Standard transport</strong> - JSON-RPC, works the same everywhere</li>
                    </ul>
                    <p>If someone builds an MCP server for GitHub, any MCP-compatible agent can use it. No custom integration. The value isn't "tools" - it's the <em>standardization</em> of how tools get exposed and connected.</p>

                    <h4>What A2A Actually Does</h4>
                    <p>A2A, introduced by Google and now under the Linux Foundation, is a standard for agents to talk to <em>other agents</em>. Not tools - agents. Entities with their own reasoning, their own tools, their own way of solving problems.</p>
                    <p>When you call an A2A agent, you describe what you want. You don't tell it how. The weather agent might use an API, scrape a website, or ask another agent. You don't know and you don't care. You get the result.</p>

                    <h4>The Critical Difference</h4>
                    <p>With <strong>MCP</strong>, you're connecting to services. You get standardized access to tools and data, but you still orchestrate. You decide what to call, when, and how to use the results.</p>
                    <p>With <strong>A2A</strong>, you're delegating to another agent. You describe the outcome you want to an autonomous entity. That entity figures out how to deliver it - possibly using its own MCP connections, possibly calling other A2A agents, possibly doing something you never anticipated.</p>
                    <p>MCP standardizes the <em>plug</em>. A2A standardizes the <em>conversation</em>.</p>

                    <h4>Why Both Exist (And Why You Need Both)</h4>
                    <p>These aren't competitors. They're complementary.</p>
                    <p><strong>MCP handles vertical integration</strong> - connecting an agent to data sources, APIs, and functions it needs to do its job.</p>
                    <p><strong>A2A handles horizontal integration</strong> - connecting agents to each other across organizational boundaries.</p>
                    <p>Real example: I built a research agent that answers questions by searching the web and synthesizing results. Internally, it uses tools (MCP-style) to search DuckDuckGo and call an LLM. Externally, it exposes an A2A endpoint so any other agent can ask it questions without knowing how it works inside.</p>
                    <p>The agent <em>uses</em> tools. The agent <em>is</em> an A2A endpoint.</p>

                    <h4>The Enterprise Play</h4>
                    <p>Here's where A2A gets interesting at scale. Imagine:</p>
                    <ul>
                        <li>Salesforce exposes a "customer lookup" agent</li>
                        <li>SAP exposes an "inventory check" agent</li>
                        <li>Stripe exposes a "payment processing" agent</li>
                    </ul>
                    <p>Your internal agent needs to handle a customer order. Instead of integrating with three different APIs (each with their own auth, SDKs, rate limits), it just asks three A2A agents. Same protocol. Same message format. The specialists handle their domains.</p>
                    <p>This is why Google pushed A2A and the Linux Foundation adopted it. It's not about building better agents - it's about making agents interoperable across the entire ecosystem.</p>

                    <h4>What I Learned Building an A2A Agent</h4>
                    <p>I built a demo to test this. My AdamOS Research Agent exposes an A2A endpoint, and I had it work alongside public agents from the A2A registry (yes, there's already a registry with 100+ agents).</p>
                    <p>The aha moment: my agent and a "Hello World" agent on Render.com and a business AI on Lifie.ai all spoke the same protocol. I discovered their capabilities via their Agent Cards, called them with identical message formats, and got responses back. Different platforms, different implementations, same interface.</p>
                    <p>That's the promise. HTTP standardized how servers talk to each other. A2A standardizes how agents talk to each other.</p>

                    <h4>The Takeaway</h4>
                    <p>When you're building AI systems, think about:</p>
                    <p><strong>Use MCP when</strong> you need your agent to do work directly - read files, query databases, call APIs. The agent is the worker, tools are its extensions.</p>
                    <p><strong>Use A2A when</strong> you want to leverage capabilities you don't control - other organizations' agents, specialized services, domain experts. The agent is a delegator, other agents are specialists.</p>
                    <p>Or, back to the hardware store:</p>
                    <ul>
                        <li>MCP = Learn to navigate the store yourself</li>
                        <li>A2A = Ask an employee and let them handle it</li>
                    </ul>
                    <p>Both have their place. The future probably involves agents that use MCP internally while exposing A2A externally - specialists that can be hired by anyone.</p>
                    <p>We're still early. But the protocols are here, they're open, and they're already being used. Time to build.</p>
                </div>
                <a href="#" class="blog-read-more" onclick="toggleBlogPost(event)">Read Full Post</a>
            </article>

            <article class="blog-card">
                <div class="blog-meta">
                    <span class="blog-date">January 13, 2026</span>
                    <span class="blog-tag">AI Architecture</span>
                </div>
                <h3>Why Agents with Tools Beat RAG for Code Understanding</h3>
                <img src="rag-vs-tools-infographic.jpg" alt="RAG vs Tool-Augmented Agents comparison infographic" class="blog-image">
                <p class="blog-excerpt">After building both RAG systems and tool-augmented agents, I've learned that giving an agent exploration tools is far superior to traditional retrieval. Here's why chunking and embedding fundamentally breaks down for codebases - and probably for most complex domains.</p>
                <div class="blog-content">
                    <h4>The Problem with RAG for Code</h4>
                    <p>Traditional RAG works like this: chunk your documents, embed them, store in a vector database, then retrieve "relevant" chunks based on semantic similarity. For simple Q&A over documents, this works reasonably well.</p>
                    <p>But codebases aren't documents. Code has <strong>structure</strong>. Functions call other functions. Classes inherit from other classes. A variable defined on line 10 might be used on line 500. When you chunk code arbitrarily, you destroy these relationships.</p>

                    <h4>What Actually Happens with Code RAG</h4>
                    <p>Say you ask: "How does the authentication flow work?" A RAG system might retrieve:</p>
                    <ul>
                        <li>A chunk with half of the login function (the other half is in a different chunk)</li>
                        <li>Some random middleware that mentions "auth" in a comment</li>
                        <li>A test file that imports the auth module</li>
                    </ul>
                    <p>The LLM then has to hallucinate the connections between these fragments. It has no way to trace the actual execution path, see the full function signatures, or understand how components interact.</p>

                    <h4>The Tool-Augmented Approach</h4>
                    <p>Instead of pre-computing what might be relevant, give the agent tools to explore:</p>
                    <ul>
                        <li><code>list_directory_structure</code> - See the project layout</li>
                        <li><code>read_file</code> - Load complete source files</li>
                        <li><code>search_code</code> - Grep for patterns across the codebase</li>
                        <li><code>get_imports</code> - Trace dependency relationships</li>
                        <li><code>find_entry_points</code> - Locate main functions</li>
                    </ul>
                    <p>Now the agent can actually follow the code. It reads the login route, sees it calls <code>authenticate()</code>, searches for that function, reads the auth service, discovers it uses a token validator, and so on. It builds a complete mental model by <strong>exploring</strong>, not by hoping the right chunks appear.</p>

                    <h4>The Context Window Advantage</h4>
                    <p>Modern LLMs have 200k+ token context windows. That's roughly 500-1000 source files worth of code. You don't need embeddings and retrieval when you can just... read the files.</p>
                    <p>The "but what about large codebases?" objection misses the point. Even in a million-line codebase, understanding any particular feature rarely requires reading more than 20-30 files. The agent with tools can identify and load exactly those files.</p>

                    <h4>Results from the Codebase Onboarding Agent</h4>
                    <p>I built a tool-augmented agent to test this thesis. The results:</p>
                    <ul>
                        <li><strong>96.7% accuracy</strong> across 10 diverse repositories in 5 languages</li>
                        <li><strong>0% hallucination rate</strong> - every claim cites specific file:line references</li>
                        <li><strong>100% tool usage</strong> - the agent explores rather than guesses</li>
                    </ul>
                    <p>No embedding infrastructure. No vector database. No chunking strategy debates. Just tools and a capable model.</p>

                    <h4>Beyond Code: A General Principle</h4>
                    <p>This insight extends beyond codebases. Any domain with <strong>structure</strong> and <strong>relationships</strong> will suffer from RAG's chunking problem:</p>
                    <ul>
                        <li>Legal documents that reference other sections</li>
                        <li>Technical specifications with dependencies</li>
                        <li>Research papers with citations and methodology chains</li>
                        <li>Database schemas with foreign key relationships</li>
                    </ul>
                    <p>Whenever understanding requires following connections, tools beat retrieval.</p>

                    <h4>The Takeaway</h4>
                    <p>RAG was a clever solution when context windows were tiny and we needed ways to "cheat" information into the prompt. But we're in a different era now. Large context windows + tool use + capable reasoning = agents that can actually explore and understand, rather than pattern-match against pre-computed embeddings.</p>
                    <p>Give your agents tools to explore. Let them discover. The results speak for themselves.</p>
                </div>
                <a href="#" class="blog-read-more" onclick="toggleBlogPost(event)">Read Full Post</a>
            </article>
        </div>
    </section>

    <section id="about" class="about">
        <div class="section-header">
            <h2>About Me</h2>
        </div>
        <div class="about-content">
            <div class="about-text">
                <p>I'm an AI Engineer with a non-linear path through tech. Dual degrees in Mechanical Engineering and Computer Science taught me to decompose complex problems systematically. Earned my OSCP, explored blockchain security, and now I'm building production AI systems.</p>
                <p>My portfolio includes 7 AI projects—multi-agent orchestration, RAG systems with hallucination detection, and LLM fine-tuning. The Codebase Onboarding Agent achieved 96.7% task success rate with zero hallucinations through systematic evaluation.</p>
                <p>I build from first principles. AI is reshaping everything, and I want to be one of the people who helps shape how it impacts the world. Currently seeking AI/ML engineering roles in Saskatoon or remote.</p>
            </div>
            <div class="about-education">
                <h3>Education</h3>
                <div class="education-item">
                    <strong>B.Sc. Computer Science</strong>
                    <span>University of Saskatchewan, 2019</span>
                </div>
                <div class="education-item">
                    <strong>B.Eng. Mechanical Engineering</strong>
                    <span>University of Saskatchewan, 2015</span>
                </div>
            </div>
            <div class="about-certs">
                <h3>Certifications</h3>
                <div class="cert-badges">
                    <span class="cert-badge">OSCP</span>
                    <span class="cert-badge">Security+</span>
                    <span class="cert-badge">Network+</span>
                    <span class="cert-badge">AI/ML Certificate</span>
                </div>
            </div>
        </div>
    </section>

    <section id="contact" class="contact">
        <div class="section-header">
            <h2>Get In Touch</h2>
            <p>Open to AI/ML engineering opportunities. Based in Calgary, AB (open to Saskatoon, remote, or hybrid).</p>
        </div>
        <div class="contact-links">
            <a href="mailto:simonar.adam@gmail.com" class="contact-card">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect width="20" height="16" x="2" y="4" rx="2"/><path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"/></svg>
                <span>simonar.adam@gmail.com</span>
            </a>
            <a href="https://ca.linkedin.com/in/adam-simonar-4b266417b" target="_blank" class="contact-card">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg>
                <span>LinkedIn</span>
            </a>
            <a href="https://github.com/Sim-Security" target="_blank" class="contact-card">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
                <span>GitHub</span>
            </a>
            <a href="https://twitter.com/SimS3curity" target="_blank" class="contact-card">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
                <span>@SimS3curity</span>
            </a>
        </div>
    </section>

    <footer class="footer">
        <p>Built with care. Open to opportunities.</p>
    </footer>

    <script>
        function toggleBlogPost(event) {
            event.preventDefault();
            const link = event.target;
            const card = link.closest('.blog-card');
            const content = card.querySelector('.blog-content');
            const excerpt = card.querySelector('.blog-excerpt');

            content.classList.toggle('expanded');
            link.classList.toggle('expanded');

            if (content.classList.contains('expanded')) {
                link.textContent = 'Show Less';
                excerpt.style.display = 'none';
            } else {
                link.textContent = 'Read Full Post';
                excerpt.style.display = 'block';
            }
        }
    </script>
</body>
</html>
